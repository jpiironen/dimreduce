% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ispca.R
\name{ispca}
\alias{ispca}
\title{Iterative supervised principal components}
\usage{
ispca(
  x,
  y,
  nctot = NULL,
  ncsup = NULL,
  exclude = NULL,
  nthresh = NULL,
  thresh = NULL,
  window = 500,
  verbose = TRUE,
  min_score = 1e-04,
  normalize = FALSE,
  center = TRUE,
  scale = TRUE,
  permtest = TRUE,
  permtest_type = "max-marginal",
  alpha = 0.1,
  perms = 500,
  method = "svd",
  ...
)
}
\arguments{
\item{x}{The original feature matrix, columns denoting the features and rows the instances.}

\item{y}{A vector with the observed target values we try to predict using \code{x}.
Can be factor for classification problems.}

\item{nctot}{Total number of latent features to extract.}

\item{ncsup}{Maximum number of latent features to extract that use supervision.}

\item{exclude}{Columns (variables) in x to ignore when extrating the new features.}

\item{nthresh}{Number of evaluations when finding the optimal screening threshold at each supervised
iteration. Increasing this number can make the supervised iterations more accurate but also increases
the computation time.}

\item{thresh}{Instead of specifying \code{nthresh}, one can specify the candidate screening thresholds
explicitly. These are numbers between 0 and 1 and are relative to the highest univariate score.
By default seq(0, 1-eps, len=nthresh) where eps = 1e-6.}

\item{window}{Maximum number of features to consider when computing each supervised component.
Lowering this number makes the computation faster, but can make the algorithm less accurate
if there are more potentially relevant features than this number.}

\item{verbose}{Whether to print some messages along the way.}

\item{min_score}{Terminate the computation at the latest when the maximum univariate score drops
below this.}

\item{normalize}{Whether to scale the extracted features so that they all have standard deviation
of one.}

\item{center}{Whether to center the original features before the computation.}

\item{scale}{Whether to scale the original features to have unit variance before the computation.}

\item{permtest}{Whether to use permutation test to decide the number of supervised components.}

\item{permtest_type}{Either 'max-marginal' or 'marginal'.}

\item{alpha}{Significance level used in the permutation test to decide whether to continue
supervised iteration.}

\item{perms}{Number of permutations to estimate the p-values for univariate scores.}

\item{method}{Method to compute the principal components. Either 'svd' or 'power'.
'power' can sometimes be slightly faster but in some cases can have very slow convergence.}

\item{...}{Currently ignored.}
}
\value{
ispca-object that is similar in spirit to the object returned by \code{\link[stats]{prcomp}}.
The object will have the following elements:
\describe{
 \item{\code{w}}{The projection (or rotation) matrix W, that transforms the original data
 \eqn{X} into the new features \eqn{Z = X W} .}
 \item{\code{z}}{The extracted latent features corresponding to the training inputs \eqn{X}.}
 \item{\code{v}}{Matrix \eqn{V} that is used to compute \eqn{W}. The columns of \eqn{V} indicate
 which variables become active at each iteration (see the paper below for more information).}
 \item{\code{sdev}}{Standard deviations of the new features.}
 \item{\code{ncsup}}{How many supervised components were extracted (the rest are computed
 in an unsupervised manner).}
 \item{\code{centers}}{Mean values for the original variables.}
 \item{\code{scales}}{Scales of the original variables.}
 \item{\code{exclude}}{Excluded variables.}
}
}
\description{
Computes dimension reduction based on the iterative supervised principal components
algorithm.
}
\section{References}{


Piironen, J. and Vehtari, A. (2018). Iterative supervised principal components.
In \emph{Proceedings of the 21st International Conference on Artificial
Intelligence and Statistics (AISTATS) PMLR 84: 106-114}.
}

\examples{
\donttest{
###

# load data
data("ovarian", package = "dimreduce")
x <- ovarian$x
y <- ovarian$y

# dimension reduction
dr <- ispca(x, y, nctot = 2)
z <- predict(dr, x) # the latent features
}

}
